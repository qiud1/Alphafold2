{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eedec855-48c9-47df-9a96-c99b6e62656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as du\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sidechainnet as scn\n",
    "import random\n",
    "import sklearn\n",
    "import einops\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80adece-2499-45bb-9275-0632cbdaf960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SidechainNet was loaded from ./sidechainnet_data/sidechainnet_casp7_30.pkl.\n"
     ]
    }
   ],
   "source": [
    "data = scn.load(casp_version=7, with_pytorch=\"dataloaders\", \n",
    "                seq_as_onehot=True, aggregate_model_input=False,\n",
    "               batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b94d25bc-a054-46b4-bbb3-c7a5945394ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_features(batch):\n",
    "    '''\n",
    "    Take a batch of sequence info and return the sequence (one-hot),\n",
    "    evolutionary info and (phi, psi, omega) angles per position, \n",
    "    as well as position mask.\n",
    "    Also return the distance matrix, and distance mask.\n",
    "    '''\n",
    "    str_seqs = batch.str_seqs # seq in str format\n",
    "    seqs = batch.seqs # seq in one-hot format\n",
    "    int_seqs = batch.int_seqs # seq in int format\n",
    "    masks = batch.msks # which positions are valid\n",
    "    lengths = batch.lengths # seq length\n",
    "    evos = batch.evos # PSSM / evolutionary info\n",
    "    angs = batch.angs[:,:,0:2] # torsion angles: phi, psi\n",
    "    \n",
    "    # use coords to create distance matrix from c-beta\n",
    "    # except use c-alpha for G\n",
    "    # coords[:, 4, :] is c-beta, and coords[:, 1, :] is c-alpha\n",
    "    coords = batch.crds # seq coord info (all-atom)\n",
    "    batch_xyz = []\n",
    "    for i in range(coords.shape[0]):\n",
    "        xyz = []\n",
    "        xyz = [coords[i][cpos+4,:] \n",
    "                if masks[i][cpos//14] and str_seqs[i][cpos//14] != 'G'\n",
    "                else coords[i][cpos+1,:]\n",
    "                for cpos in range(0, coords[i].shape[0]-1, 14)]\n",
    "        batch_xyz.append(torch.stack(xyz))\n",
    "    batch_xyz = torch.stack(batch_xyz)\n",
    "    # now create pairwise distance matrix\n",
    "    dmats = torch.cdist(batch_xyz, batch_xyz)\n",
    "    # create matrix mask (0 means i,j invalid)\n",
    "    dmat_masks = torch.einsum('bi,bj->bij', masks, masks)\n",
    "    \n",
    "    return seqs, evos, angs, masks, dmats, dmat_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6969eac3-0163-479e-bc59-ae546aa2d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, in_dim = 256, d_k = 16):\n",
    "        '''\n",
    "        Represents an attention head for multihead attention,\n",
    "        d_k is 16 by default.\n",
    "        in_dim is 256 by default.\n",
    "        '''\n",
    "        super(AttentionHead, self).__init__()\n",
    "        \n",
    "        self.d_k = d_k\n",
    "        #create query, key, and values\n",
    "        self.q = nn.Linear(in_dim, d_k)\n",
    "        self.k = nn.Linear(in_dim, d_k)\n",
    "        self.v = nn.Linear(in_dim, d_k)\n",
    "        \n",
    "    def forward(self, sequence, bias, row_or_col):\n",
    "        '''\n",
    "        Given a sequence in MSA_rep of size n_res x 256, calculate attention.\n",
    "        Depending on row_or_col, bias is either added or excluded.\n",
    "        '''\n",
    "        query = self.q(sequence)\n",
    "        key = self.k(sequence)\n",
    "        value = self.v(sequence)\n",
    "        \n",
    "        A_sh = torch.matmul(query, torch.transpose(key, 1, 2))/np.sqrt(self.d_k)\n",
    "        if row_or_col == \"row\":\n",
    "            A_sh += bias.squeeze(dim = -1)\n",
    "        \n",
    "        #take softmax with respect to the rows\n",
    "        A_sh = F.softmax(A_sh, dim = 1)\n",
    "        A_sh = torch.matmul(A_sh, value)\n",
    "        \n",
    "        return A_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fd7ddac-b3c4-4539-b0ce-8a46314de992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Row_Col_Attention(nn.Module):\n",
    "    '''\n",
    "    compute either row-wise or column-wise attention depending on the given argument.\n",
    "    '''\n",
    "    def __init__(self, row_or_col, num_heads):\n",
    "        super(Row_Col_Attention, self).__init__()\n",
    "        \n",
    "        #define multi head attention\n",
    "        self.mha = nn.ModuleList([AttentionHead() for i in range(num_heads)])\n",
    "        \n",
    "        #create a gate for each head, corresponding to each index.\n",
    "        #a gate maps msa_rep to 1 and sigmoids it to determine how much information is kept from a head.\n",
    "        self.gates =  nn.ModuleList([nn.Sequential(nn.Linear(256, 1), nn.Sigmoid()) for i in range(num_heads)])\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.row_or_col = row_or_col\n",
    "        \n",
    "        #linear layer to project the new msa_rep into 256 dim\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        \n",
    "    def forward(self, msa_rep, bias):\n",
    "        if self.row_or_col == \"col\":\n",
    "            #transpose the msa_rep if we are doing column wise attention\n",
    "            msa_rep = torch.transpose(msa_rep, 1, 2)\n",
    "        \n",
    "        #calculate all the respective gates dot attention head outputs.\n",
    "        gated_outs = []\n",
    "        for s in range(msa_rep.shape[1]):\n",
    "            for i in range(self.num_heads):\n",
    "                outputs = self.mha[i](msa_rep[:,s,:,:], bias, self.row_or_col)\n",
    "                gate = self.gates[i](msa_rep).squeeze()\n",
    "                gate_out = torch.transpose(gate,1,2)*outputs\n",
    "                gated_outs.append(gate_out)          \n",
    "        \n",
    "        #concatenate them to form O_sh\n",
    "        O_sh = torch.concat(gated_outs, dim = 2)\n",
    "        new_msa_rep = rearrange(O_sh, 'b i (c j) -> b c i j', c = 16)\n",
    "        print(msa_rep.shape, new_msa_rep.shape)\n",
    "        new_msa_rep = self.fc1(new_msa_rep)\n",
    "        \n",
    "        return new_msa_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54e4847d-665b-48b7-b762-30e7a379d46c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Outer_Prod_Mean(nn.Module):\n",
    "    '''\n",
    "    Finds the outer product mean between the pair-wise representation\n",
    "    and the msa representation.\n",
    "    The output is a n_res x n_res x 128 pair_rep\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Outer_Prod_Mean, self).__init__()\n",
    "        #linear layer to project i[s] and j[s] to 32 dim\n",
    "        self.fc1 = nn.Linear(256, 32)\n",
    "        \n",
    "        #flatten the mean outer product to C*C\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        #linear layer to project the outer product mean to 128 dim\n",
    "        self.fc2 = nn.Linear(32, 128)\n",
    "        \n",
    "    def forward(self, msa_rep):\n",
    "        #iterate through clusters, pick slices i and j and project them into 32 dim and gather their outer products\n",
    "        for i in range(msa_rep.shape[2]):\n",
    "            outer_prods = [torch.outer(self.fc1(msa_rep[i][s]), self.fc1(msa_rep[j][s]))\n",
    "                           for j in range(msa_rep.shape[2]) for s in range(msa_rep.shape[1])]\n",
    "        \n",
    "        #concatenate all o_ij to make the output and take the mean\n",
    "        new_pair_rep = torch.mean(torch.concat(outer_prods, dim = 2))\n",
    "        new_pair_rep = self.flatten(new_pair_rep)\n",
    "        \n",
    "        #project to n_res x n_res x 128 dim\n",
    "        new_pair_rep = self.fc2(new_pair_rep)\n",
    "        \n",
    "        #make sure to do residual connection after calling the function\n",
    "        return new_pair_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d02c25d7-afaa-4f00-a41f-e22aa3479522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mult_Attention(nn.Module):\n",
    "    def __init__(self, out = True):\n",
    "        '''\n",
    "        Does incoming(default) multiplicative attention on a given pair_rep.\n",
    "        out: set to False to do incoming attention\n",
    "        '''\n",
    "        super(Mult_Attention, self).__init__()\n",
    "        self.out = out\n",
    "        self.ln = nn.LayerNorm(128)\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        \n",
    "        self.gate1 = nn.Sequential(nn.Linear(128, 128), nn.Sigmoid())\n",
    "        self.gate2 = nn.Sequential(nn.Linear(128, 128), nn.Sigmoid())\n",
    "        self.gate3 = nn.Sequential(nn.Linear(128, 128), nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, pair_rep):\n",
    "        #Do a layer norm on pair_rep\n",
    "        pair_rep = self.ln(pair_rep)\n",
    "        \n",
    "        #make A and B\n",
    "        A = self.fc1(pair_rep)\n",
    "        B = self.fc2(pair_rep)\n",
    "        \n",
    "        #Make gates for A and B\n",
    "        gate_A = self.gate1(pair_rep)\n",
    "        gate_B = self.gate2(pair_rep)\n",
    "        gate_Z = self.gate3(pair_rep)\n",
    "        \n",
    "        #take dot product of A, B and their gates\n",
    "        new_A = torch.dot(A, gate_A)\n",
    "        new_B = torch.dot(B, gate_B)\n",
    "        \n",
    "        #transpose a and b if we are doing incoming attention\n",
    "        if not self.out:\n",
    "            new_A = torch.transpose(new_A, 1, 2)\n",
    "            new_B = torch.transpose(new_B, 1, 2)\n",
    "            \n",
    "        #prepare to do tensorwise dot product of all slices\n",
    "        new_A = torch.tile(new_A, (1,1,1,128))\n",
    "        new_B = torch.tile(new_B, (1,1,1,128))\n",
    "        new_B = torch.transpose(new_B, 1, 2)\n",
    "        \n",
    "        #Find the dot product of all slices and sum\n",
    "        out = torch.dot(new_A, new_B)\n",
    "        out = torch.sum(out, 1)\n",
    "        \n",
    "        #project to n_res x n_res x 128\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        #gate the output\n",
    "        gated_out = torch.dot(out, gate_Z)\n",
    "        \n",
    "        return gated_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd194cd3-79af-4562-b175-a5231d924968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tri_Attention(nn.Module):\n",
    "    '''\n",
    "    Does starting triangular attention by default.\n",
    "    ending: set to true to do ending triangular attention\n",
    "    '''\n",
    "    def __init__(self, ending = False, c = 128, num_heads = 4):\n",
    "        super(Tri_Attention, self).__init__()\n",
    "        self.ending = ending\n",
    "        self.num_heads = num_heads\n",
    "        self.c = c\n",
    "        \n",
    "        self.q = nn.ModuleList([nn.Linear(128, 32) for i in range(num_heads)])\n",
    "        self.k = nn.ModuleList([nn.Linear(128, 32) for i in range(num_heads)])\n",
    "        self.v = nn.ModuleList([nn.Linear(128, 32) for i in range(num_heads)])\n",
    "        self.b = nn.ModuleList([nn.Linear(128, 1) for i in range(num_heads)])\n",
    "        self.g = nn.ModuleList([nn.Sequential(nn.Linear(128,128), nn.Sigmoid()) for i in range(num_heads)])\n",
    "        \n",
    "        self.fc1 = nn.Linear(32, 128)\n",
    "        \n",
    "    def forward(self, pair_rep):\n",
    "        output = []\n",
    "        for h in range(self.num_heads):\n",
    "            query = self.q[h](pair_rep)\n",
    "            key = self.k[h](pair_rep)\n",
    "            value = self.v[h](pair_rep)\n",
    "            bias = self.b[h](pair_rep)\n",
    "            gate = self.g[h](pair_rep)\n",
    "            \n",
    "            '''\n",
    "            for i in range(query.shape[1]):\n",
    "                for j in range(query.shape[2]):\n",
    "                    for k in range(query.shape[3]):\n",
    "                        a = torch.matmul(torch.transpose(query[i,j], 1, 2), key[i,k])/torch.sqrt(self.c) + bias[j,k]\n",
    "                        a = F.softmax(a, dim=-1)\n",
    "                        a = torch.sum(a*value[j,k], dim=-1)*g[i,j]\n",
    "                        output.append(a)\n",
    "            '''\n",
    "            \n",
    "            a = torch.matmul(q, torch.transpose(key, 1, 2))/np.sqrt(self.c) + torch.transpose(bias, 1, 2)\n",
    "            a = F.softmax(a, dim = 1)\n",
    "            a *= value\n",
    "            out = a * g\n",
    "            output.append(out)\n",
    "        \n",
    "        #concat all outputs\n",
    "        output = torch.concat(output, 1)\n",
    "        output = self.fc1(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6df0f741-c8fc-4e5e-858b-6a635e630516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evoformer(nn.Module):\n",
    "    def __init__(self, n_clust=16, num_heads=8, device='cpu'):\n",
    "        '''\n",
    "        Creates the MSA_representation and the Z(pairwise) matrix given a PSSM and a sequence.\n",
    "        n_clust: number of PSSMs.\n",
    "        num_heads: number of attention heads(8 by default)\n",
    "        '''\n",
    "        super(Evoformer, self).__init__()\n",
    "        \n",
    "        self.n_clust = n_clust\n",
    "        self.num_heads = num_heads\n",
    "        self.device = device\n",
    "        \n",
    "        #linear layers to project evos into n_clust x n_res x 256\n",
    "        self.fc0 = nn.ModuleList([nn.Linear(21, 256) for i in range(n_clust)])\n",
    "        #linear layer to project seqs to n_res x 256\n",
    "        self.fc1 = nn.Linear(20, 256)\n",
    "        #linear layer to project seqs to n_res x 128\n",
    "        self.fc2 = nn.Linear(20, 128)\n",
    "        self.fc3 = nn.Linear(20, 128)\n",
    "        #Linear layer to project distances into 128 space\n",
    "        self.fc4 = nn.Linear(64, 128)\n",
    "        #linear layer to project pair_rep to bias\n",
    "        self.fc5 = nn.Linear(128, 1)\n",
    "        #linear layer to project the single representation to 256 dim\n",
    "        self.fc6 = nn.Linear(128, 256)\n",
    "        #linear layer to project the single representation to 384 dim\n",
    "        self.fc7 = nn.Linear(256, 384)\n",
    "        \n",
    "        #define the transitional layers to pass the new msa_rep through\n",
    "        self.transition = nn.Sequential(nn.Linear(256, 1024), nn.ReLU(), nn.Linear(1024, 256))\n",
    "        \n",
    "        #define all attentions\n",
    "        self.row_att = Row_Col_Attention(\"row\", self.num_heads)\n",
    "        self.col_att = Row_Col_Attention(\"col\", self.num_heads)\n",
    "        self.mul_att_in = Mult_Attention(out = False)\n",
    "        self.mul_att_out = Mult_Attention(out = True)\n",
    "        self.tri_att_start = Tri_Attention(ending = False)\n",
    "        self.tri_att_end = Tri_Attention(ending = True)\n",
    "        \n",
    "        #define outer_product_mean\n",
    "        self.out_prod_mean = Outer_Prod_Mean()\n",
    "        \n",
    "    \n",
    "    def create_msa_rep(self, evos, seqs):\n",
    "        '''\n",
    "        Create the msa_representation given evolutionary data evos\n",
    "        and the seqs, both are n_res x 21.\n",
    "        '''\n",
    "        #obtain n_clust layers of PSSM(evos); stack them into a (n_clust x n_res x 256) matrix\n",
    "        clusters = [self.fc0[i](evos) for i in range(self.n_clust)]\n",
    "        msa_rep = torch.stack(clusters, dim=1)\n",
    "        \n",
    "        #project the seqs from n_res x 21 to n_res x 256 and tile it.\n",
    "        new_seqs = self.fc1(seqs)\n",
    "        new_seqs = new_seqs.unsqueeze(dim=1)\n",
    "        new_seqs = torch.tile(new_seqs, (1, self.n_clust, 1, 1))\n",
    "        \n",
    "        #add the seqs to the msa_rep\n",
    "        msa_rep += new_seqs\n",
    "        \n",
    "        return msa_rep\n",
    "    \n",
    "    def create_pair_rep(self, seqs):\n",
    "        '''\n",
    "        Create pair_wise representations given seqs.\n",
    "        '''\n",
    "        #create the pairwise rep matrix\n",
    "        a_i = self.fc2(seqs).unsqueeze(dim=2)\n",
    "        b_j = self.fc3(seqs).unsqueeze(dim=2)\n",
    "        a_i = torch.tile(a_i, (1, 1, a_i.shape[1], 1))\n",
    "        b_j = torch.tile(b_j, (1, 1, b_j.shape[1], 1))\n",
    "        pair_rep = a_i + torch.transpose(b_j, 1, 2)\n",
    "        \n",
    "        #add the relative position rel_pos\n",
    "        idx_j = torch.arange(0, seqs.shape[1]).unsqueeze(dim=1)\n",
    "        idx_j = torch.tile(idx_j, (1, idx_j.shape[1]))\n",
    "        idx_i = torch.transpose(idx_j, 0, 1)\n",
    "        # idx_i , idx_j = idx_i.to(device), idx_j.to(device)\n",
    "        dist_ij = idx_i - idx_j   \n",
    "        bins = torch.linspace(-32, 32, 64)\n",
    "        dist_ij = torch.bucketize(dist_ij, bins)\n",
    "        dist_ij[dist_ij>=64] = 63\n",
    "        dist_ij = dist_ij.unsqueeze(dim=0)\n",
    "        dist_ij = torch.tile(dist_ij, (pair_rep.shape[0], 1, 1))\n",
    "        dist_ij = F.one_hot(dist_ij).type(torch.float)\n",
    "        dist_ij = dist_ij.to(self.device)\n",
    "        # print(dist_ij.shape)\n",
    "        rel_pos = self.fc4(dist_ij)\n",
    "        pair_rep += rel_pos\n",
    "        return pair_rep\n",
    "    \n",
    "    def create_bias(self, pair_rep):\n",
    "        '''\n",
    "        given the pairwise representation create the bias\n",
    "        '''\n",
    "        bias = self.fc5(pair_rep)\n",
    "        return bias\n",
    "        \n",
    "    def single_rep(self, msa_rep):\n",
    "        '''\n",
    "        Find the singular representation of M\n",
    "        Should only be done on the last block.\n",
    "        '''\n",
    "        single_rep = self.fc6(msa_rep[:,1,:,:])\n",
    "        single_rep = self.fc7(single_rep)\n",
    "        return single_rep  \n",
    "    \n",
    "    def forward(self, seqs, evos):\n",
    "        #create msa_rep, pair_rep, bias\n",
    "        msa_rep = self.create_msa_rep(evos, seqs)\n",
    "        pair_rep = self.create_pair_rep(seqs)\n",
    "        bias = self.create_bias(pair_rep)\n",
    "        \n",
    "        #feed msa_rep into row -> col -> transition\n",
    "        msa_rep = msa_rep + self.row_att(msa_rep, bias) \n",
    "        msa_rep = msa_rep + self.col_att(msa_rep, bias)\n",
    "        msa_rep = msa_rep + self.transition(msa_rep) #output of evoformer for msa_rep\n",
    "        \n",
    "        #do the outer product mean\n",
    "        pair_rep = pair_rep + self.out_prod_mean(msa_rep)\n",
    "        \n",
    "        #do triangular attention\n",
    "        pair_rep = pair_rep + self.mult_att_out(pair_rep)\n",
    "        pair_rep = pair_rep + self.mult_att_in(pair_rep)\n",
    "        pair_rep = pair_rep + self.tri_att_start(pair_rep)\n",
    "        pair_rep = pair_rep + self.tri_att_end(pair_rep)\n",
    "        \n",
    "        #do the transition\n",
    "        pair_rep = pair_rep + self.transition(pair_rep) #output of evoformer for pair_rep\n",
    "        \n",
    "        return msa_rep, pair_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b33c7b74-6d7b-4367-8086-e5398fc856db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Evoformer(\n",
       "  (fc0): ModuleList(\n",
       "    (0): Linear(in_features=21, out_features=256, bias=True)\n",
       "    (1): Linear(in_features=21, out_features=256, bias=True)\n",
       "    (2): Linear(in_features=21, out_features=256, bias=True)\n",
       "    (3): Linear(in_features=21, out_features=256, bias=True)\n",
       "    (4): Linear(in_features=21, out_features=256, bias=True)\n",
       "    (5): Linear(in_features=21, out_features=256, bias=True)\n",
       "    (6): Linear(in_features=21, out_features=256, bias=True)\n",
       "    (7): Linear(in_features=21, out_features=256, bias=True)\n",
       "    (8): Linear(in_features=21, out_features=256, bias=True)\n",
       "    (9): Linear(in_features=21, out_features=256, bias=True)\n",
       "    (10): Linear(in_features=21, out_features=256, bias=True)\n",
       "    (11): Linear(in_features=21, out_features=256, bias=True)\n",
       "    (12): Linear(in_features=21, out_features=256, bias=True)\n",
       "    (13): Linear(in_features=21, out_features=256, bias=True)\n",
       "    (14): Linear(in_features=21, out_features=256, bias=True)\n",
       "    (15): Linear(in_features=21, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=20, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=20, out_features=128, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (fc5): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (fc6): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (fc7): Linear(in_features=256, out_features=384, bias=True)\n",
       "  (transition): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  )\n",
       "  (row_att): Row_Col_Attention(\n",
       "    (mha): ModuleList(\n",
       "      (0): AttentionHead(\n",
       "        (q): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "      (1): AttentionHead(\n",
       "        (q): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "      (2): AttentionHead(\n",
       "        (q): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "      (3): AttentionHead(\n",
       "        (q): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "      (4): AttentionHead(\n",
       "        (q): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "      (5): AttentionHead(\n",
       "        (q): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "      (6): AttentionHead(\n",
       "        (q): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "      (7): AttentionHead(\n",
       "        (q): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (gates): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "  )\n",
       "  (col_att): Row_Col_Attention(\n",
       "    (mha): ModuleList(\n",
       "      (0): AttentionHead(\n",
       "        (q): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "      (1): AttentionHead(\n",
       "        (q): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "      (2): AttentionHead(\n",
       "        (q): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "      (3): AttentionHead(\n",
       "        (q): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "      (4): AttentionHead(\n",
       "        (q): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "      (5): AttentionHead(\n",
       "        (q): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "      (6): AttentionHead(\n",
       "        (q): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "      (7): AttentionHead(\n",
       "        (q): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (k): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (v): Linear(in_features=256, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (gates): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "  )\n",
       "  (mul_att_in): Mult_Attention(\n",
       "    (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (gate1): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (gate2): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (gate3): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (mul_att_out): Mult_Attention(\n",
       "    (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (gate1): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (gate2): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (gate3): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (tri_att_start): Tri_Attention(\n",
       "    (q): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (1): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "    )\n",
       "    (k): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (1): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "    )\n",
       "    (v): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (1): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "    )\n",
       "    (b): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (1): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (3): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "    (g): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
       "  )\n",
       "  (tri_att_end): Tri_Attention(\n",
       "    (q): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (1): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "    )\n",
       "    (k): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (1): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "    )\n",
       "    (v): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (1): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "    )\n",
       "    (b): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (1): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (3): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "    (g): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
       "  )\n",
       "  (out_prod_mean): Outer_Prod_Mean(\n",
       "    (fc1): Linear(in_features=256, out_features=32, bias=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (fc2): Linear(in_features=32, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "epochs = 50\n",
    "learning_rate = 0.0001\n",
    "n_clust = 16\n",
    "\n",
    "model = Evoformer(n_clust, device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "model = model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6a78894-7d51-4726-99b0-521cec455498",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 16, 256, 256]) torch.Size([9, 16, 256, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (16) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18356/1211440286.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mevo_crop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mddmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscretized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_crop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevo_crop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_18356/1304833905.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seqs, evos)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m#feed msa_rep into row -> col -> transition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mmsa_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsa_rep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow_att\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsa_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mmsa_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsa_rep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol_att\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsa_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mmsa_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsa_rep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsa_rep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#output of evoformer for msa_rep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_18356/3711829175.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, msa_rep, bias)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsa_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow_or_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsa_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mgate_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mgated_outs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (16) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "for epoch in range(1,epochs+1):\n",
    "    for batch in data['train']:\n",
    "        seqs, evos, angs, masks, dmats, dmat_masks = get_seq_features(batch)\n",
    "        seqs, evos, angs, masks, dmats, dmat_masks = seqs.to(device), evos.to(device), angs.to(device), masks.to(device), dmats.to(device), dmat_masks.to(device)\n",
    "        \n",
    "        #generate a random starting index\n",
    "        start_idx = random.randint(1,16)\n",
    "        \n",
    "        seqs = F.pad(seqs, (0, 0, 0, 256 - (seqs.shape[1] - start_idx)%256), 'constant', 0)\n",
    "        evos = F.pad(evos, (0, 0, 0, 256 - (evos.shape[1] - start_idx)%256), 'constant', 0)\n",
    "        \n",
    "        #discretize the matrix\n",
    "        bins = torch.linspace(2,22, 64)\n",
    "        bins = bins.to(device)\n",
    "        discretized = torch.clamp(dmats, min = 2, max = 22)\n",
    "        discretized = torch.bucketize(discretized, bins, right = True)\n",
    "        discretized = F.pad(discretized, (0, 256-(discretized.shape[2] - start_idx)%256, 0, 256-(discretized.shape[1] - start_idx)%256, 0, 0), 'constant', 0)\n",
    "        \n",
    "        for i in range(start_idx, seqs.shape[1], 128):\n",
    "            seq_crop = seqs[:,i:i+256, :]\n",
    "            evo_crop = evos[:,i:i+256, :]\n",
    "            ddmat = discretized[:,i:i+256, i:i+256]\n",
    "            pred = model(seq_crop.type(torch.float), evo_crop)\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fee7f0-c741-45bf-bc85-9026a1354aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
